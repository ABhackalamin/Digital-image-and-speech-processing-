{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "# Read the audio file\n",
    "y, fs = librosa.load('/content/drive/MyDrive/harvard.wav', sr=None)\n",
    "\n",
    "# Define frame size and overlap (in samples)\n",
    "frame_size = 256\n",
    "overlap = 128\n",
    "\n",
    "# Calculate number of frames\n",
    "num_frames = (len(y) - frame_size) // (frame_size - overlap) + 1\n",
    "\n",
    "# Initialize variables\n",
    "voiced_frames = []\n",
    "unvoiced_frames = []\n",
    "silence_frames = []\n",
    "\n",
    "# Iterate through each frame\n",
    "for i in range(num_frames):\n",
    "    # Extract current frame\n",
    "    start_idx = i * (frame_size - overlap)\n",
    "    end_idx = start_idx + frame_size\n",
    "    frame = y[start_idx:end_idx]\n",
    "\n",
    "    # Calculate energy of the frame\n",
    "    energy = np.sum(np.abs(frame)**2)\n",
    "\n",
    "    # Calculate zero-crossing rate (ZCR)\n",
    "    zcr = np.sum(np.diff(np.sign(frame)) != 0)\n",
    "\n",
    "    # Thresholds for voiced, unvoiced, and silence detection\n",
    "    voiced_threshold = 0.01 * np.max(energy)  # adjust threshold based on your audio\n",
    "    unvoiced_threshold = 0.001 * np.max(energy)  # adjust threshold based on your audio\n",
    "    silence_threshold = 0.0001 * np.max(energy)  # adjust threshold based on your audio\n",
    "\n",
    "    # Identify frame type based on energy and ZCR\n",
    "    if energy > voiced_threshold and zcr > 10:  # adjust values for voiced detection\n",
    "        voiced_frames.append(i)\n",
    "    elif energy > unvoiced_threshold and zcr < 10:  # adjust values for unvoiced detection\n",
    "        unvoiced_frames.append(i)\n",
    "    else:\n",
    "        silence_frames.append(i)\n",
    "\n",
    "# Calculate time axis for plotting\n",
    "time_axis = np.arange(len(y)) / fs\n",
    "\n",
    "# Plot the original signal separately\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot original signal\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(time_axis, y, 'k')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Original Signal')\n",
    "\n",
    "# Plot voiced segments\n",
    "plt.subplot(4, 1, 2)\n",
    "for i in voiced_frames:\n",
    "    start_idx = i * (frame_size - overlap)\n",
    "    end_idx = start_idx + frame_size\n",
    "    plt.plot(time_axis[start_idx:end_idx], y[start_idx:end_idx], 'g')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Voiced Regions')\n",
    "\n",
    "# Plot unvoiced segments\n",
    "plt.subplot(4, 1, 3)\n",
    "for i in unvoiced_frames:\n",
    "    start_idx = i * (frame_size - overlap)\n",
    "    end_idx = start_idx + frame_size\n",
    "    plt.plot(time_axis[start_idx:end_idx], y[start_idx:end_idx], 'r')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Unvoiced Regions')\n",
    "\n",
    "# Plot silence segments\n",
    "plt.subplot(4, 1, 4)\n",
    "for i in silence_frames:\n",
    "    start_idx = i * (frame_size - overlap)\n",
    "    end_idx = start_idx + frame_size\n",
    "    plt.plot(time_axis[start_idx:end_idx], y[start_idx:end_idx], 'b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Silence Regions')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
